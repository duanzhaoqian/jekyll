---
layout: post
title:  "hadoop2.7.1编译安装"
date:   2016-05-07
desc: "hadoop2.7.1编译安装"
keywords: "Linux,Hadoop"
categories: [Database]
tags: [hadoop]
icon: fa-linux
---

# hadoop 2.7.1

<!--
create time: 2016-05-07 10:00:09
Author: <段朝骞>

This file is created by Marboo<http://marboo.io> template file $MARBOO_HOME/.media/starts/default.md
本文件由 Marboo<http://marboo.io> 模板文件 $MARBOO_HOME/.media/starts/default.md 创建
-->

# 编译安装
## 编译安装hadoop 2.7.11. 下载文件	jdk-7u80-linux-x64.tar.gz	hadoop-2.7.1-src.tar.gz		protobuf-2.5.0.tar.gz	apache-ant-1.9.6-bin.tar.gz		cmake-3.4.2-Linux-x86_64.tar.gz		apache-maven-3.3.9-bin.tar.gz	
findbugs-noUpdateChecks-3.0.1.tar.gz	![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h0.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h0.png)2. yum安装依赖

	```bash	yum -y install  lzo-devel  zlib-devel  gcc autoconf automake libtool glibc-headers gcc-c++ openssl-devel
	```3. 编译protobuf

	```bash	cd protobuf-2.5.0	./configure	make	make check	make install
	```
	4. 解压文件，设置环境变量

	```bash	tar -zxvf *	vim /etc/profile	export JAVA_HOME=/usr/java	export PATH=$JAVA_HOME/bin:$PATH	export MAVEN_HOME=/root/soft/apache-maven-3.3.9	export CMAKE_HOME=/root/soft/cmake-3.4.2-Linux-x86_64	export FINDBUGS_HOME=/root/soft/findbugs-3.0.1	export ANT_HOME=/root/soft/apache-ant-1.9.6	export PATH=$PATH:$MAVEN_HOME/bin:$CMAKE_HOME/bin:$FINDBUGS_HOME/	bin:$ANT_HOME/bin
	```![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h1.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h1.png)5. 使环境变量生效

	```bash	source /etc/profile
	```
	6. 进入hadoop 目录
	
	```bash	cd hadoop-2.7.1-src
	```7. mvn编译代码（自动下载maven依赖，消耗时间过长）

	```bash	mvn clean package -DskipTests -Pdist,native,docs -Dtar
	```
		***注意：编译完成需要消耗6.5G硬盘***	![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h2.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h2.png)
	## 启动Hadoop方案：	准备hd201 hd202 hd205 hd206 hd207 hd208 hd209 hd210 8台机器	
分别配置各自hostname，hosts文件	配置ssh免密码登录	hd201 hd202 运行NameNode ResourceManager		hd205 hd206 hd207 运行DataNode NodeManager	hd208 hd209 hd210运行zookeeper JournalNode	### ssh免密码登录

生成公私钥

```bashssh-keygen
```![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h3.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h3.png)
默认使用rsa，可以加参数 -t dsa
![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h4.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h4.png)复制id_rsa.pub 里的内容到 ~/.ssh/authorized_keys

```bashcd ~/.sshcat id_rsa.pub >> authorized_keys
```
修改authorized_keys权限为644

```bashchmod 644 authorized_keys
```![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h5.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h5.png)复制斜公钥到其它机器

```bashssh-copy-id -i hd202ssh-copy-id -i hd205ssh-copy-id -i hd206ssh-copy-id -i hd207...
```### 配置各机器hostname

```bashvim /etc/sysconfig/network
```![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h6.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h6.png)### 配置/etc/hosts

```192.168.1.201 hd201192.168.1.202 hd202192.168.1.203 hd203192.168.1.204 hd204192.168.1.205 hd205192.168.1.206 hd206192.168.1.207 hd207192.168.1.208 hd208192.168.1.209 hd209192.168.1.210 hd210
```### 配置hadoop-env.sh	

```export JAVA_HOME=/usr/java
```![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h7.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h7.png)### 配置core-site.xml

```xml<configuration>    <property>        <name>fs.defaultFS</name>        <value>hdfs://mycluster</value>    </property>    <property>        <name>hadoop.tmp.dir</name>        <value>/hadoop/tmp</value>    </property> <property>       <name>ha.zookeeper.quorum</name>       <value>hd210:2181,hd209:2181,hd208:2181</value></property></configuration>```
![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h8.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h8.png)### 配置hdfs-site.xml		
```xml<configuration><!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 --><property>  <name>dfs.nameservices</name>  <value>mycluster</value></property><!-- mycluster下面有两个NameNode，分别是nn1，nn2 --><property>  <name>dfs.ha.namenodes.mycluster</name>  <value>nn1,nn2</value></property><!-- nn1的RPC通信地址 --><property>  <name>dfs.namenode.rpc-address.mycluster.nn1</name>  <value>hd201:8020</value></property><!-- nn2的RPC通信地址 --><property>  <name>dfs.namenode.rpc-address.mycluster.nn2</name>  <value>hd202:8020</value></property><!-- nn1的http通信地址 --><property>  <name>dfs.namenode.http-address.mycluster.nn1</name>  <value>hd201:50070</value></property><!-- nn2的http通信地址 --><property>  <name>dfs.namenode.http-address.mycluster.nn2</name>  <value>hd202:50070</value></property><!-- 指定NameNode的元数据在JournalNode上的存放位置 --><property>  <name>dfs.namenode.shared.edits.dir</name> <value>qjournal://hd210:8485;hd209:8485;hd208:8485/mycluster</value></property><!-- 配置失败自动切换实现方式 --><property>  <name>dfs.client.failover.proxy.provider.mycluster</name>  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value></property><!-- 配置隔离机制 --><property>      <name>dfs.ha.fencing.methods</name>      <value>sshfence</value>    </property><!-- 使用隔离机制时需要ssh免登陆，注意id_rsa位置 -->    <property>      <name>dfs.ha.fencing.ssh.private-key-files</name>      <value>/root/.ssh/id_rsa</value>    </property><!-- 指定JournalNode在本地磁盘存放数据的位置 --><property>  <name>dfs.journalnode.edits.dir</name>  <value>/hadoop/journal/data</value></property><!-- 开启NameNode失败自动切换 --><property>   <name>dfs.ha.automatic-failover.enabled</name>   <value>true</value> </property> </configuration>
```

取消文件权限控制(在hdfs-site.xml中添加)，可以不加，使用-DHADOOP_USER_NAME=root启动应用程序

```xml<property>    <name>dfs.permissions.enabled</name>    <value>false</value>    <description>       If "true", enable permission checking in HDFS.       If "false", permission checking is turned off,       but all other behavior is unchanged.       Switching from one parameter value to the other does not change the mode,       owner or group of files or directories.    </description> </property>```
 ### 配置datanode的配置文件slaves

```bashvim slaveshd205hd206hd207
```
### 配置mapreduce文件mapred-site.xml默认是没有mapred-site.xml文件的，里面有一个mapred-site.xml.example,重命名为mapred-site.xml

```bashmv mapred-site.xml.example mapred-site.xml
```
配置内容如下，这里就是指明mapreduce是用在YARN之上来执行的。

```xml<configuration>    <property>        <name>mapreduce.framework.name</name>        <value>yarn</value>        <description>Execution framework set to Hadoop YARN.</description>    </property></configuration>```
![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h9.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h9.png)### 配置yarn-site.xml

```xml<configuration><!-- 开启RM高可用 --><property>   <name>yarn.resourcemanager.ha.enabled</name>   <value>true</value></property><!-- 指定RM的cluster id --><property>   <name>yarn.resourcemanager.cluster-id</name>   <value>duanzq</value></property><!-- 指定RM的名字 --><property>   <name>yarn.resourcemanager.ha.rm-ids</name>   <value>rm1,rm2</value></property><!-- 分别指定RM的地址 --><property>   <name>yarn.resourcemanager.hostname.rm1</name>   <value>hd201</value></property><property>   <name>yarn.resourcemanager.hostname.rm2</name>   <value>hd202</value></property><!-- 指定zk集群地址 --><property>   <name>yarn.resourcemanager.zk-address</name>   <value>hd210:2181,hd209:2181,hd208:2181</value></property><property>   <name>yarn.nodemanager.aux-services</name>   <value>mapreduce_shuffle</value></property></configuration>```
![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h10.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h10.png)### 将hadoop复制到其它节点

```bashscp -r /hadoop hd202:/scp -r /hadoop hd203:/scp -r /hadoop hd206:/scp -r /hadoop hd207:/scp -r /hadoop hd208:/scp -r /hadoop hd209:/scp -r /hadoop hd210:/
```
### 启动zookeeper		启动hd208，hd209，hd210 三台机器上的zookeeper		
### 在hd201上启动journalnode

```bashsbin/hadoop-daemons.sh start journalnode
```
在hd208，hd209，hd210上会启动journalnode节点![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h11.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h11.png)QuorumPeerMain是zookeeper		### 在hd201上格式化hadoop		

```bashhadoop namenode -format
```
格式化后会在根据core-site.xml中的hadoop.tmp.dir配置生成个文件，在hd201中会出现一个tmp文件夹，/hadoop/tmp，现在规划的是hadoop01和hadoop02是NameNode，然后将/soft/hadoop-2.7.1/tmp拷贝到hadoop02:/soft/hadoop-2.7.1/下，这样hd202就不用格式化了。

```bashscp -r /hadoop/tmp hd202: /hadoop/
```
### 在hd201上格式化ZK

```bashhdfs zkfc -formatZK
```
### 在hd201上启动HDFS

```bashsbin/start-dfs.sh
```
### 在hd201上启动YARN

```bashsbin/start-yarn.sh
```
### 单独启动NameNode

```bashsbin/hadoop-daemon.sh start namenode
```
### 单独启动ResourceManager

```bashsbin/yarn-daemon.sh start resourcemanager
```
## 访问url		http://192.168.1.201:50070/		http://192.168.1.201:8088/		启动historyserver		

```bashsbin/mr-jobhistory-daemon.sh start historyserver
```
http://192.168.1.201:19888/![https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h12.png](https://raw.githubusercontent.com/duanzhaoqian/pic/master/hadoop/h12.png)## 参考地址[http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html)# 各端口说明

Hadoop集群的各部分一般都会使用到多个端口，有些是daemon之间进行交互之用，有些是用于RPC访问以及HTTP访问。而随着Hadoop周边组件的增多，完全记不住哪个端口对应哪个应用，特收集记录如此，以便查询。这里包含我们使用到的组件：HDFS, YARN, HBase, Hive, ZooKeeper:
组件|节点|默认端口|配置|用途说明
----|----|----|----|----
HDFS|DataNode|50010|dfs.datanode.address|datanode服务端口，用于数据传输
HDFS|DataNode|50075|dfs.datanode.http.address|http服务的端口
HDFS|DataNode|50475|dfs.datanode.https.address|https服务的端口
HDFS|DataNode|50020|dfs.datanode.ipc.address|ipc服务的端口
HDFS|NameNode|50070|dfs.namenode.http-address|http服务的端口
HDFS|NameNode|50470|dfs.namenode.https-address|https服务的端口
HDFS|NameNode|8020|fs.defaultFS|接收Client连接的RPC端口，用于获取文件系统metadata信息。
HDFS|journalnode|8485|dfs.journalnode.rpc-address|RPC服务
HDFS|journalnode|8480|dfs.journalnode.http-address|HTTP服务
HDFS|ZKFC|8019|dfs.ha.zkfc.port|ZooKeeper FailoverController，用于NN HAYARN|ResourceManager|8032|yarn.resourcemanager.address|RM的applications manager(ASM)端口
YARN|ResourceManager|8030|yarn.resourcemanager.scheduler.address|scheduler组件的IPC端口YARN|ResourceManager|8031|yarn.resourcemanager.resource-tracker.address|IPCYARN|ResourceManager|8033|yarn.resourcemanager.admin.address|IPCYARN|ResourceManager|8088|yarn.resourcemanager.webapp.address|http服务端口
YARN|NodeManager|8040|yarn.nodemanager.localizer.address|localizer IPC
YARN|NodeManager|8042|yarn.nodemanager.webapp.address|http服务端口YARN|NodeManager|8041|yarn.nodemanager.address|NM中container manager的端口YARN|JobHistory Server|10020|mapreduce.jobhistory.address|IPCYARN|JobHistory Server|19888|mapreduce.jobhistory.webapp.address|http服务端口HBase|Master|60000|hbase.master.port|IPCHBase|Master|60010|hbase.master.info.port|http服务端口HBase|RegionServer|60020|hbase.regionserver.port|IPCHBase|RegionServer|60030|hbase.regionserver.info.port|http服务端口HBase|HQuorumPeer|2181|hbase.zookeeper.property.clientPort|HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。HBase|HQuorumPeer|2888|hbase.zookeeper.peerport|HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。HBase|HQuorumPeer|3888|hbase.zookeeper.leaderport|HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。Hive|Metastore|9083|/etc/default/hive-metastore中export PORT=<port>来更新默认端口Hive|HiveServer|10000|/etc/hive/conf/hive-env.sh中export HIVE_SERVER2_THRIFT_PORT=<port>来更新默认端口ZooKeeper|Server|2181|/etc/zookeeper/conf/zoo.cfg中clientPort=<port>|对客户端提供服务的端口ZooKeeper|Server|2888|/etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分|follower用来连接到leader，只在leader上监听该端口。ZooKeeper|Server|3888|/etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分|用于leader选举的。只在electionAlg是1,2或3(默认)时需要。所有端口协议均基于TCP。	对于存在Web UI（HTTP服务）的所有hadoop daemon，有如下url：	/logs 		日志文件列表，用于下载和查看		/logLevel 		允许你设定log4j的日志记录级别，类似于hadoop daemonlog		/stacks 		所有线程的stack trace，对于debug很有帮助		/jmx 		服务端的Metrics，以JSON格式输出。		/jmx?qry=Hadoop:*会返回所有hadoop相关指标。 		/jmx?get=MXBeanName::AttributeName 查询指定bean指定属性的值，例如/jmx?get=Hadoop:service=NameNode,name=NameNodeInfo::ClusterId会返回ClusterId。	
	这个请求的处理类：org.apache.hadoop.jmx.JMXJsonServlet		而特定的Daemon又有特定的URL路径特定相应信息。		NameNode:http://:50070/		/dfshealth.jsp 		HDFS信息页面，其中有链接可以查看文件系统		/dfsnodelist.jsp?whatNodes=(DEAD|LIVE) 		显示DEAD或LIVE状态的datanode		/fsck 		运行fsck命令，不推荐在集群繁忙时使用！		DataNode:http://:50075/		/blockScannerReport 		每个datanode都会指定间隔验证块信息		